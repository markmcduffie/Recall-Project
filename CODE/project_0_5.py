# -*- coding: utf-8 -*-
"""Project 0.5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hf8OOQUV7XkshurDCFHBQcMX_JPp9BI5
"""

!pip install ta

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
import scipy.stats as stats

import pandas as pd
import numpy as np
from google.colab import files

import ta
from ta import add_all_ta_features
from ta.utils import dropna

from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split
from sklearn.metrics import accuracy_score, classification_report, r2_score, mean_squared_error
from sklearn.linear_model import LinearRegression, PoissonRegressor
from sklearn.preprocessing import StandardScaler, normalize, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree



uploaded = files.upload()

### UTILITY FUNCTIONS

# read stocks data from CSVs
def getStockData(path):

    df = pd.read_csv(path)
    df['Date'] = pd.to_datetime(df['Date'], utc=True)

    return df


# standardize values in array
def standardize_data(ds):
    sc = StandardScaler()
    sc.fit(ds)
    res = sc.transform(ds)

    return res


# Get Next-Day Close
def getNextDayClose(df):

  close = df['Close'][1:len(df['Close'])]

  return close


# Get Standard Deviation
# using 30-day window
def getStdDev(df, window=30):
    res = df[['Date', 'Close']]
    res.insert(0, 'stdDev', df['Close'].rolling(window).std())

    return res

### INDICATOR FUNCTIONS

# Get Stochastic Oscillator
def getSTO(df):

  res_raw = (
       ta.momentum.StochasticOscillator(
          high = df['High'],
          low = df['Low'],
          close = df['Close']
        )
      .stoch()
      .values.reshape(-1,1)
  )

  res = standardize_data(res_raw)

  return res


# Get Relative Strength Index
def getRSI(df):

  res_raw = (
       ta.momentum.RSIIndicator(
          df['Close'],
        )
      .rsi()
      .values.reshape(-1,1)
  )

  res = standardize_data(res_raw)

  return res


# Get MACD
def getMAC(df):

  res_raw = (
       ta.trend.MACD(
          df['Close'],
        )
      .macd()
      .values.reshape(-1,1)
  )

  res = standardize_data(res_raw)

  return res


# Get Aroon Indicator
def getARI(df):

  res_raw = (
       ta.trend.AroonIndicator(
          high = df['High'],
          low = df['Low']
        )
      .aroon_indicator()
      .values.reshape(-1,1)
  )

  res = standardize_data(res_raw)

  return res


# Get Average Directional Index
def getADX(df):

  res_raw = (
       ta.trend.ADXIndicator(
          df['High'],
          df['Low'],
          df['Close']
        )
      .adx()
      .values.reshape(-1,1)
  )

  res = standardize_data(res_raw)

  return res


# Get Accumulation/Distribution Index
def getADI(df):

  res_raw = (
       ta.volume.acc_dist_index(
          df['High'],
          df['Low'],
          df['Close'],
          df['Volume'],
        )
      .values.reshape(-1,1)
  )

  res = standardize_data(res_raw)

  return res


# Get On-Balance Volume Indicator
def getOBV(df):

  res_raw = (
      ta.volume.OnBalanceVolumeIndicator(df['Close'], df['Volume'])
      .on_balance_volume()
      .values.reshape(-1,1)
  )

  res = standardize_data(res_raw)

  return res

### MAIN

# Files stored in https://drive.google.com/drive/folders/1m8i0sSAy6VHuo-p9Qux_BrxKeT5iKien?usp=drive_link
stocks = ['F_2016.csv',
          'GM_2016.csv',
          'HMC_2016.csv',
          'MBGAF_2016.csv',
          'TM_2016.csv',
          'VWAGY_2016.csv']

# Add recall data
recall_data = pd.read_csv('recall.csv')
# Remove duplicate data from recall csv (from 1367 rows to 937)
recall_data = recall_data.drop_duplicates(subset=['Date', 'Manufacturer'], keep='first')
recall_data['Date'] = pd.to_datetime(recall_data['Date'], utc=True)
recall_data.rename(columns={'Manufacturer': 'Company'}, inplace=True)

model_data = {}

for stock in stocks:
  df_stock = getStockData(stock)

  # merge df_stock and recall_data
  df_stock = df_stock.merge(recall_data, on=['Date', 'Company'], how='left')

  # fill in missing recall data based on previous date
  df_stock['Recall.Factor'] = df_stock['Recall.Factor'].ffill()

  close = getNextDayClose(df_stock)

  ind_obv = getOBV(df_stock)
  ind_adi = getADI(df_stock)
  ind_adx = getADX(df_stock)
  ind_ari = getARI(df_stock)
  ind_mac = getMAC(df_stock)
  ind_rsi = getRSI(df_stock)
  ind_sto = getSTO(df_stock)

  # getStdDev( dataframe with 'Date' and 'Close' columns, rolling window length = 30 )
  # use .dropna() to discard rows with na values
  stdDev = getStdDev(df_stock, 30)

  # drop last record as it will not have a next-day value
  x = len(df_stock)-1

  df = pd.DataFrame({
      'obv': ind_obv[0:x, 0],
      'adi': ind_adi[0:x, 0],
      'adx': ind_adx[0:x, 0],
      'ari': ind_ari[0:x, 0],
      'mac': ind_mac[0:x, 0],
      'rsi': ind_rsi[0:x, 0],
      'sto': ind_sto[0:x, 0],
      'next_close': close,
      }, columns=['obv', 'adi', 'adx', 'ari', 'mac', 'rsi', 'sto', 'Recall.Factor', 'next_close', 'stdDev', 'Date']);

  df['stdDev'] = pd.Series(stdDev['stdDev'])
  df['Date'] = pd.Series(stdDev['Date'].dt.strftime('%m/%d/%Y'))
  df['Recall.Factor'] = pd.Series(df_stock['Recall.Factor'])
  df = df.dropna()

  model_data[stock] = df

"""# Recall Indicator

"""

#Distribution is visualized through R. code for the output file can be found in recall_indicator.RMD

"""# Linear Regression"""

### Linear Regression Models w/o Recall Indicator

stock_models = {}

# To store actuals vs predicted for visualization
y_test_dict = {}
y_pred_dict = {}

# To store R2 scores for later analysis
stock_r2_without = {}

print("Without Recall Indicator")

for stock in stocks:
    df = model_data[stock]

    X = df[['obv', 'adi', 'adx', 'ari', 'mac', 'rsi', 'sto']]
    y = df['next_close']

    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

    # Create linear regression model
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Predict and evaluate the model
    y_pred = model.predict(X_test)

    # Store y_test & y_pred
    y_test_dict[stock] = y_test
    y_pred_dict[stock] = y_pred

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    stock_r2_without[stock] = r2

    print(f"Stock: {stock}, MSE: {mse}, R2 Score: {r2}")

    # Store the model
    stock_models[stock] = model

### Linear Regression Models w/ Recall Indicator

recall_stock_models = {}

y_test_dict_recall = {}
y_pred_dict_recall = {}

# To store R2 scores for later analysis
stock_r2_with = {}

print("With Recall Indicator")

for stock in stocks:
    df = model_data[stock]

    X = df[['obv', 'adi', 'adx', 'ari', 'mac', 'rsi', 'sto', 'Recall.Factor']]
    y = df['next_close']

    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

    # Create linear regression model
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Predict and evaluate the model
    y_pred = model.predict(X_test)

    # Store y_test & y_pred
    y_test_dict_recall[stock] = y_test
    y_pred_dict_recall[stock] = y_pred

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    stock_r2_with[stock] = r2

    print(f"Stock: {stock}, MSE: {mse}, R2 Score: {r2}")

    # Store the model
    recall_stock_models[stock] = model

### Statistical Significance Analysis

n = 1733
p_full = 8
p_reduced = 7

f_test_results = {}

for stock in stocks:
    r2_full = stock_r2_with[stock]
    r2_reduced = stock_r2_without[stock]

    numerator = ((r2_full - r2_reduced) / (p_full - p_reduced))
    denominator = ((1 - r2_full) / (n - p_full - 1))
    f_stat = numerator / denominator
    df1 = p_full - p_reduced
    df2 = n - p_full - 1
    p_value = 1 - stats.f.cdf(f_stat, df1, df2)

    f_test_results[stock] = {'F-statistic': f_stat, 'p-value': p_value}

for stock, results in f_test_results.items():
    print(f"Stock: {stock}, F-statistic: {results['F-statistic']}, p-value: {results['p-value']}")

### Combine results into a single dataframe for use in Tableau
combined_results = []

# Without Recall Indicator
for stock in stocks:
    df_no_recall = pd.DataFrame({
        'Stock': stock.replace('_2016.csv', ''),
        'Actual_Close': y_test_dict[stock].tolist(),
        'Predicted_Close': y_pred_dict[stock].tolist(),
        'Indicator': 'Without Recall',
        'R2_Score': stock_r2_without[stock],
        'MSE': mean_squared_error(y_test_dict[stock], y_pred_dict[stock])
    }).reset_index(drop=True)
    combined_results.append(df_no_recall)

# With Recall Indicator
for stock in stocks:
    df_with_recall = pd.DataFrame({
        'Stock': stock.replace('_2016.csv', ''),
        'Actual_Close': y_test_dict_recall[stock].tolist(),
        'Predicted_Close': y_pred_dict_recall[stock].tolist(),
        'Indicator': 'With Recall',
        'R2_Score': stock_r2_with[stock],
        'MSE': mean_squared_error(y_test_dict_recall[stock], y_pred_dict_recall[stock])
    }).reset_index(drop=True)
    combined_results.append(df_with_recall)

# Combine all results into a single DataFrame
combined_results_df = pd.concat(combined_results, ignore_index=True)

# Export to csv
combined_results_df.to_csv('combined_stock_results_recall_indicator.csv', index=False)

# Download csv
files.download('combined_stock_results_recall_indicator.csv')

"""# Volatility"""

### Technical Indicator and Volitity Correlation Analysis

# Correlation function
def correlation_calc(df, indicator):
  correlation_matrix = np.corrcoef(df[indicator], df['stdDev'])
  correlation = correlation_matrix[0, 1]
  return correlation

# To store correlation data
correlation_data = {}

# Loop through each stock and calculate correlation coefficients
for stock, df, in model_data.items():
  print(f"Correlation for {stock}:")
  technical_indicators = ['Recall.Factor', 'obv', 'adi', 'adx', 'ari', 'mac', 'rsi', 'sto']
  correlation_data[stock] = {}
  for indicator in technical_indicators:
    correlation_data[stock][indicator] = correlation_calc(df, indicator)
    print(f"Correlation between {indicator} and stock's standard deviation: {correlation_data[stock][indicator]:.3f}")
  print()

# Bar charts
for stock, data in correlation_data.items():
  df = pd.DataFrame(list(data.items()), columns=['Indicator', 'Correlation'])

  matplotlib.pyplot.figure(figsize=(10, 6))
  sns.barplot(x='Indicator', y='Correlation', data=df)
  matplotlib.pyplot.title(f'Correlation Coefficients for {stock}')
  matplotlib.pyplot.xticks(rotation=45)
  matplotlib.pyplot.xlabel('Technical Indicator')
  matplotlib.pyplot.ylabel('Correlation Coefficient')
  matplotlib.pyplot.show()

### Consolidate correlation data for use in Tableau
consolidated_correlation_data = []

# Loop through each stock and append to list
for stock, indicators in correlation_data.items():
  for indicator, correlation in indicators.items():
    consolidated_correlation_data.append({
        'Stock': stock.replace('_2016.csv', ''),
        'Indicator': indicator,
        'Correlation': correlation
    })

correlation_df = pd.DataFrame(consolidated_correlation_data)

# Export to csv
correlation_df.to_csv('correlation_results.csv', index=False)

# Download csv
files.download('correlation_results.csv')

### RANDOM FOREST MODEL
### training w/ recall indicator by default

for stock in stocks:

  df = model_data[stock]

  (df_train, df_test) = train_test_split(df, test_size=0.3, shuffle = False, random_state = 31337)

  ### train with recall factor
  x_train = df_train[['obv', 'adi', 'adx', 'ari', 'mac', 'rsi', 'sto', 'Recall.Factor']]
  x_test = df_test[['obv', 'adi', 'adx', 'ari', 'mac', 'rsi', 'sto', 'Recall.Factor']]

  ### uncomment to train without recall factor
  # x_train = df_train[['obv', 'adi', 'adx', 'ari', 'mac', 'rsi', 'sto']]
  # x_test = df_test[['obv', 'adi', 'adx', 'ari', 'mac', 'rsi', 'sto']]


  y_train = df_train[['next_close']]
  y_test = df_test[['next_close']]
  direction_labels_train = y_train['next_close'].shift(-1).ge(y_train['next_close'])
  direction_labels_test = y_test['next_close'].shift(-1).ge(y_test['next_close'])

  rf = RandomForestClassifier()

  param_grid = {'n_estimators': [15, 25, 35, 60, 100], 'max_depth': [3, 4, 5, 6, 7]}
  gscv_rfc = GridSearchCV(estimator = rf, param_grid = param_grid, error_score='raise')
  gscv_rfc.fit(x_train, direction_labels_train.values.flatten())

  best_params = gscv_rfc.best_params_

  rf_best = RandomForestClassifier(
      n_estimators = best_params['n_estimators'],
      max_depth = best_params['max_depth']
  )

  rf_best.fit(x_train, direction_labels_train.values.flatten())
  y_predict_train = rf_best.predict(x_train)
  y_predict_test = rf_best.predict(x_test)

  train_accuracy = accuracy_score(direction_labels_train, y_predict_train)
  test_accuracy = accuracy_score(direction_labels_test, y_predict_test)

  print("train accuracy: ", train_accuracy, " & test accuracy: ", test_accuracy)

  mse = mean_squared_error(y_test, y_predict_test)
  r2 = r2_score(y_test, y_predict_test)

  print(f"Stock: {stock}, MSE: {mse}, R2 Score: {r2}")

"""# ARIMA"""

#ARIMA
from statsmodels.tsa.arima.model import ARIMA
import math
from statsmodels.tsa.arima.model import ARIMA
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import plot_predict
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})

# pd.options.mode.chained_assignment = None  # default='warn'

# DATA

df_F_sd = model_data['F_2016.csv']
df_F_sd = df_F_sd[['Date', 'stdDev']]
df_F_sd['Date'] = pd.to_datetime(df_F_sd['Date'])

df_GM_sd = model_data['GM_2016.csv']
df_GM_sd = df_GM_sd[['Date', 'stdDev']]
df_GM_sd['Date'] = pd.to_datetime(df_GM_sd['Date'])

df_HMC_sd = model_data['HMC_2016.csv']
df_HMC_sd = df_HMC_sd[['Date', 'stdDev']]
df_HMC_sd['Date'] = pd.to_datetime(df_HMC_sd['Date'])

df_MBGAF_sd = model_data['MBGAF_2016.csv']
df_MBGAF_sd = df_MBGAF_sd[['Date', 'stdDev']]
df_MBGAF_sd['Date'] = pd.to_datetime(df_MBGAF_sd['Date'])

df_TM_sd = model_data['TM_2016.csv']
df_TM_sd = df_TM_sd[['Date', 'stdDev']]
df_TM_sd['Date'] = pd.to_datetime(df_TM_sd['Date'])

df_VWAGY_sd = model_data['VWAGY_2016.csv']
df_VWAGY_sd = df_VWAGY_sd[['Date', 'stdDev']]
df_VWAGY_sd['Date'] = pd.to_datetime(df_VWAGY_sd['Date'])

!pip install pmdarima
import pmdarima as pm
from pmdarima.model_selection import train_test_split
from bokeh.plotting import figure, show, output_notebook
from statsmodels.tsa.arima_model import ARIMA

#Ford
auto_F = pm.auto_arima(df_F_sd.stdDev, start_p=1, start_q=1,
                      test='adf',       # use adftest to find optimal 'd'
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=None,           # let model determine 'd'
                      seasonal=False,   # No Seasonality
                      start_P=0,
                      D=0,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=True)

display(auto_F.summary())

next_30_F = auto_F.predict(n_periods=30)

df_F_sd_index = df_F_sd[['stdDev']]
next_30_F = next_30_F.to_frame()
next_30_F.rename(columns={next_30_F.columns[0]: "stdDev" }, inplace = True)

combine_F = pd.concat([df_F_sd_index , next_30_F])
combine_F

#GM
auto_GM = pm.auto_arima(df_GM_sd.stdDev, start_p=1, start_q=1,
                      test='adf',       # use adftest to find optimal 'd'
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=None,           # let model determine 'd'
                      seasonal=False,   # No Seasonality
                      start_P=0,
                      D=0,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=True)

display(auto_GM.summary())

next_30_GM = auto_GM.predict(n_periods=30)

df_GM_sd_index = df_GM_sd[['stdDev']]
next_30_GM = next_30_GM.to_frame()
next_30_GM.rename(columns={next_30_GM.columns[0]: "stdDev" }, inplace = True)

combine_GM = pd.concat([df_GM_sd_index , next_30_GM])
combine_GM

#HMC
auto_HMC = pm.auto_arima(df_HMC_sd.stdDev, start_p=1, start_q=1,
                      test='adf',       # use adftest to find optimal 'd'
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=None,           # let model determine 'd'
                      seasonal=False,   # No Seasonality
                      start_P=0,
                      D=0,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=True)

display(auto_HMC.summary())

next_30_HMC = auto_HMC.predict(n_periods=30)

df_HMC_sd_index = df_HMC_sd[['stdDev']]
next_30_HMC = next_30_HMC.to_frame()
next_30_HMC.rename(columns={next_30_HMC.columns[0]: "stdDev" }, inplace = True)

combine_HMC = pd.concat([df_HMC_sd_index , next_30_HMC])
combine_HMC

#MBGAF
auto_MBGAF = pm.auto_arima(df_MBGAF_sd.stdDev, start_p=1, start_q=1,
                      test='adf',       # use adftest to find optimal 'd'
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=None,           # let model determine 'd'
                      seasonal=False,   # No Seasonality
                      start_P=0,
                      D=0,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=True)

display(auto_MBGAF.summary())

next_30_MBGAF = auto_MBGAF.predict(n_periods=30)

df_MBGAF_sd_index = df_MBGAF_sd[['stdDev']]
next_30_MBGAF = next_30_MBGAF.to_frame()
next_30_MBGAF.rename(columns={next_30_MBGAF.columns[0]: "stdDev" }, inplace = True)

combine_MBGAF = pd.concat([df_MBGAF_sd_index , next_30_MBGAF])
combine_MBGAF

#TM
auto_TM = pm.auto_arima(df_TM_sd.stdDev, start_p=1, start_q=1,
                      test='adf',       # use adftest to find optimal 'd'
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=None,           # let model determine 'd'
                      seasonal=False,   # No Seasonality
                      start_P=0,
                      D=0,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=True)

display(auto_TM.summary())

next_30_TM = auto_TM.predict(n_periods=30)

df_TM_sd_index = df_TM_sd[['stdDev']]
next_30_TM = next_30_TM.to_frame()
next_30_TM.rename(columns={next_30_TM.columns[0]: "stdDev" }, inplace = True)

combine_TM = pd.concat([df_TM_sd_index , next_30_TM])
combine_TM

#VWAGY
auto_VWAGY = pm.auto_arima(df_VWAGY_sd.stdDev, start_p=1, start_q=1,
                      test='adf',       # use adftest to find optimal 'd'
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=None,           # let model determine 'd'
                      seasonal=False,   # No Seasonality
                      start_P=0,
                      D=0,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=True)

display(auto_VWAGY.summary())

next_30_VWAGY = auto_VWAGY.predict(n_periods=30)

df_VWAGY_sd_index = df_VWAGY_sd[['stdDev']]
next_30_VWAGY = next_30_VWAGY.to_frame()
next_30_VWAGY.rename(columns={next_30_VWAGY.columns[0]: "stdDev" }, inplace = True)

combine_VWAGY = pd.concat([df_VWAGY_sd_index , next_30_VWAGY])
combine_VWAGY

# Function to add stock identifier and data type
def add_stock_info(combined_df, stock_name, historical_length):
    # Reset index to ensure it's unique and sequential
    combined_df.reset_index(drop=True, inplace=True)

    # Add stock identifier
    combined_df['Stock'] = stock_name

    # Add data type (Historical or Forecasted)
    combined_df['Type'] = ['Historical' if i < historical_length else 'Forecasted' for i in range(len(combined_df))]

    return combined_df

# Get length of historical data for each stock
len_F = len(df_F_sd)
len_GM = len(df_GM_sd)
len_HMC = len(df_HMC_sd)
len_MBGAF = len(df_MBGAF_sd)
len_TM = len(df_TM_sd)
len_VWAGY = len(df_VWAGY_sd)

# Add stock info
combine_F = add_stock_info(combine_F, 'Ford', len_F)
combine_GM = add_stock_info(combine_GM, 'GM', len_GM)
combine_HMC = add_stock_info(combine_HMC, 'HMC', len_HMC)
combine_MBGAF = add_stock_info(combine_MBGAF, 'MBGAF', len_MBGAF)
combine_TM = add_stock_info(combine_TM, 'TM', len_TM)
combine_VWAGY = add_stock_info(combine_VWAGY, 'VWAGY', len_VWAGY)

# Combine all into a single dataframe
all_stocks_combined = pd.concat([combine_F, combine_GM, combine_HMC, combine_MBGAF, combine_TM, combine_VWAGY])
# Display the final combined dataframe
print(all_stocks_combined)

# Save the dataframe to a CSV file for use in Tableau
all_stocks_combined.to_csv("stocks_arima_analysis.csv", index=True)


# Download csv
files.download("stocks_arima_analysis.csv")

recalls = pd.read_csv('recall.csv')